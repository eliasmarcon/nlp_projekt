{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, FunnelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, matthews_corrcoef\n",
    "\n",
    "import modules.deep_learning_modules as dl_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/#31-bert-tokenizer\n",
    "#### https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "MODELTYPE = \"transformer\" \n",
    "SEED_VAL = 42\n",
    "\n",
    "\n",
    "model_mapping = {   \n",
    "                 \n",
    "    \"bert_base\": \"google-bert/bert-base-uncased\",\n",
    "    \"distilbert\":\"distilbert/distilbert-base-uncased\",\n",
    "    \"albert_base\": \"albert/albert-base-v2\",\n",
    "    \"mistral_small\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"transformer\":\"funnel-transformer/small-base\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Use the GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Tokenization & Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = dl_modules.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the Model tokenizer.\n",
    "print('Loading Model tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_mapping[MODELTYPE], do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  i hate woman\n",
      "Tokenized:  ['i', 'hate', 'woman']\n",
      "Token IDs:  [1045, 5223, 2450]\n"
     ]
    }
   ],
   "source": [
    "# example tokenization\n",
    "print('Original: ', sentences[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 443\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences:\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i hate woman\n",
      "Token IDs: tensor([ 101, 1045, 5223, 2450,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels = dl_modules.tokenize_data(sentences, labels, tokenizer, MAX_LEN)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original:', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28,134 training samples\n",
      " 9,378 validation samples\n",
      " 9,378 test samples\n",
      "\n",
      "Whole dataset size: 46890\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = dl_modules.create_data_split(input_ids, attention_masks, labels, SEED_VAL)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(train_dataset)))\n",
    "print('{:>6,} validation samples'.format(len(val_dataset)))\n",
    "print('{:>6,} test samples\\n'.format(len(test_dataset)))\n",
    "\n",
    "print('Whole dataset size:', len(train_dataset) + len(val_dataset) + len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dl_modules.create_data_loader(train_dataset, BATCH_SIZE)\n",
    "validation_dataloader = dl_modules.create_data_loader(val_dataset, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unused variables\n",
    "del sentences, labels, input_ids, attention_masks, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad616de0fc104de68da073133d8e8e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/462M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--funnel-transformer--small-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at funnel-transformer/small-base and are newly initialized: ['classifier.linear_out.bias', 'classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FunnelForSequenceClassification(\n",
       "  (funnel): FunnelBaseModel(\n",
       "    (embeddings): FunnelEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): FunnelEncoder(\n",
       "      (attention_structure): FunnelAttentionStructure(\n",
       "        (sin_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (cos_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x ModuleList(\n",
       "          (0-3): 4 x FunnelLayer(\n",
       "            (attention): FunnelRelMultiheadAttention(\n",
       "              (hidden_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (attention_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (q_head): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_head): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (post_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "            (ffn): FunnelPositionwiseFFN(\n",
       "              (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (activation_function): NewGELUActivation()\n",
       "              (activation_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (layer_norm): LayerNorm((768,), eps=1e-09, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): FunnelClassificationHead(\n",
       "    (linear_hidden): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_out): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if MODELTYPE == \"transformer\":\n",
    "    \n",
    "    model = FunnelForSequenceClassification.from_pretrained(\n",
    "        model_mapping[MODELTYPE],\n",
    "        num_labels = 2, \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    \n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_mapping[MODELTYPE],\n",
    "        num_labels = 2, \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The funnel-transformer/small-base model has 247 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "funnel.embeddings.word_embeddings.weight                (30522, 768)\n",
      "funnel.embeddings.layer_norm.weight                           (768,)\n",
      "funnel.embeddings.layer_norm.bias                             (768,)\n",
      "funnel.encoder.blocks.0.0.attention.r_w_bias                (12, 64)\n",
      "funnel.encoder.blocks.0.0.attention.r_r_bias                (12, 64)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "funnel.encoder.blocks.0.0.attention.r_kernel            (768, 12, 64)\n",
      "funnel.encoder.blocks.0.0.attention.r_s_bias                (12, 64)\n",
      "funnel.encoder.blocks.0.0.attention.seg_embed            (2, 12, 64)\n",
      "funnel.encoder.blocks.0.0.attention.q_head.weight         (768, 768)\n",
      "funnel.encoder.blocks.0.0.attention.k_head.weight         (768, 768)\n",
      "funnel.encoder.blocks.0.0.attention.k_head.bias               (768,)\n",
      "funnel.encoder.blocks.0.0.attention.v_head.weight         (768, 768)\n",
      "funnel.encoder.blocks.0.0.attention.v_head.bias               (768,)\n",
      "funnel.encoder.blocks.0.0.attention.post_proj.weight      (768, 768)\n",
      "funnel.encoder.blocks.0.0.attention.post_proj.bias            (768,)\n",
      "funnel.encoder.blocks.0.0.attention.layer_norm.weight         (768,)\n",
      "funnel.encoder.blocks.0.0.attention.layer_norm.bias           (768,)\n",
      "funnel.encoder.blocks.0.0.ffn.linear_1.weight            (3072, 768)\n",
      "funnel.encoder.blocks.0.0.ffn.linear_1.bias                  (3072,)\n",
      "funnel.encoder.blocks.0.0.ffn.linear_2.weight            (768, 3072)\n",
      "funnel.encoder.blocks.0.0.ffn.linear_2.bias                   (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "classifier.linear_hidden.weight                           (768, 768)\n",
      "classifier.linear_hidden.bias                                 (768,)\n",
      "classifier.linear_out.weight                                (2, 768)\n",
      "classifier.linear_out.bias                                      (2,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print(f'The {model_mapping[MODELTYPE]} model has {len(params)} different named parameters.\\n')\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  LEARNING_RATE, # default is 5e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed_all(SEED_VAL)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(f'======== Epoch {epoch_i + 1} / {EPOCHS} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 200 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = dl_modules.format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels).loss\n",
    "        \n",
    "        logits = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels).logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_train_accuracy += dl_modules.flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average accuracy over all of the batches.\n",
    "    avg_train_accuracy = total_train_accuracy / len(train_dataloader)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = dl_modules.format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"  Average training accuracy: {avg_train_accuracy:.2f}\")\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {training_time:}\")\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    " \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "   \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                         token_type_ids=None, \n",
    "                         attention_mask=b_input_mask,\n",
    "                         labels=b_labels).loss\n",
    "        \n",
    "            logits = model(b_input_ids,\n",
    "                           token_type_ids=None,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels).logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += dl_modules.flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Average Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = dl_modules.format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'Epoch': epoch_i + 1,\n",
    "            'Training_Loss': avg_train_loss,\n",
    "            'Training_Accuracy': avg_train_accuracy,\n",
    "            'Validation_Loss': avg_val_loss,\n",
    "            'Validation_Accuracy': avg_val_accuracy,\n",
    "            'Training_Time': training_time,\n",
    "            'Validation_Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(\"../performance_statistics\"):\n",
    "        os.makedirs(\"../performance_statistics\")\n",
    "    \n",
    "    df_stats.to_csv(f\"../performance_statistics/training_stats_{MODELTYPE}_{LEARNING_RATE}.csv\", index=True)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(dl_modules.format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "output_dir = f'../models/'\n",
    "filename = f\"model_{MODELTYPE}_{LEARNING_RATE}\"\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_dir = output_dir + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Training/Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.read_csv(f\"../performance_statistics/training_stats_{MODELTYPE}.csv\")\n",
    "df_stats = df_stats.set_index('Epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training_Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Validation_Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([np.arange(1, EPOCHS + 1)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training_Accuracy'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Validation_Accuracy'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.xticks([np.arange(1, EPOCHS + 1)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training_Time'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Validation_Time'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.legend()\n",
    "plt.xticks([np.arange(1, EPOCHS + 1)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "sentences, labels = dl_modules.read_data()\n",
    "input_ids, attention_masks, labels = dl_modules.tokenize_data(sentences, labels, tokenizer, MAX_LEN)\n",
    "_, _, test_dataset = dl_modules.create_data_split(input_ids, attention_masks, labels, SEED_VAL)\n",
    "testing_dataloader = dl_modules.create_data_loader(test_dataset, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "print(f'Predicting labels for {len(test_dataset)} test sentences...')\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "predictions_accuracy, true_labels_accuracy = [], []\n",
    "\n",
    "# Predict \n",
    "for index, batch in enumerate(testing_dataloader):\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "    \n",
    "    prediction_list = np.argmax(logits, axis=1).flatten()\n",
    "    true_labels_list = label_ids.flatten()\n",
    "    \n",
    "    for i, prediction in enumerate(prediction_list):\n",
    "        predictions_accuracy.append(prediction)\n",
    "        true_labels_accuracy.append(true_labels_list[i])\n",
    "\n",
    "\n",
    "    if index % 50 == 0:\n",
    "    \n",
    "      print(f\"Done with {index} out of {len(testing_dataloader)} batches.\")\n",
    "  \n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(predictions_accuracy, true_labels_accuracy, target_names=['not_hate', 'hate'])\n",
    "\n",
    "# Save the classification report to a file\n",
    "with open(f'../performance_statistics/classification_report_{MODELTYPE}.txt', 'w') as file:\n",
    "    file.write(report)\n",
    "\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matthews Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Positive hateful samples: {labels.sum()} of {len(labels)} ({labels.sum() / len(labels.label) * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "  \n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  \n",
    "  # Calculate and store the coef for this batch.  \n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
    "  matthews_set.append(matthews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a barplot showing the MCC score for each batch of test samples.\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "ax = sns.barplot(x = list(range(len(matthews_set)- 20, len(matthews_set))), y = matthews_set[len(matthews_set) - 20: len(matthews_set)], errorbar=None)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "plt.title('MCC Score per Batch')\n",
    "plt.ylabel('MCC Score (-1 to +1)')\n",
    "plt.xlabel('Batch #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print(f'Total MCC: {mcc:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
