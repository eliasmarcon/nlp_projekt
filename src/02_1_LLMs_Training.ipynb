{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, FunnelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import modules.deep_learning_modules as dl_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source: https://mccormickml.com/2019/07/22/BERT-fine-tuning/#31-bert-tokenizer\n",
    "#### https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELTYPE = \"bert_base\" \n",
    "EPOCHS = 1\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 6\n",
    "LEARNING_RATE = 2e-5\n",
    "DATASET = 'dataset_preprocessed_no_transformation' # dataset_preprocessed_no_transformation, dataset_preprocessed_stopwords\n",
    "SEED_VAL = 42\n",
    "\n",
    "base_path = '../datasets/01_preprocessed_datasets/' + DATASET + '.csv'\n",
    "\n",
    "model_mapping = {   \n",
    "                 \n",
    "    \"bert_base\": \"google-bert/bert-base-uncased\",\n",
    "    \"distilbert\":\"distilbert/distilbert-base-uncased\",\n",
    "    \"albert_base\": \"albert/albert-base-v2\",\n",
    "    \"roberta\": \"openai-community/roberta-base-openai-detector\",\n",
    "    \"transformer\":\"funnel-transformer/small-base\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Use the GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('Use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Tokenization & Input Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, labels = dl_modules.read_data(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the Model tokenizer.\n",
    "print('Loading Model tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_mapping[MODELTYPE], do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  i hate woman\n",
      "Tokenized:  ['i', 'hate', 'woman']\n",
      "Token IDs:  [1045, 5223, 2450]\n"
     ]
    }
   ],
   "source": [
    "# example tokenization\n",
    "print('Original: ', sentences[0])\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 443\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences:\n",
    "        \n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: i hate woman\n",
      "Token IDs: tensor([ 101, 1045, 5223, 2450,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "Label: tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive - FH Technikum Wien\\AI_Master\\2_Semester\\Natural_Language_Processing\\nlp_projekt\\src\\modules\\deep_learning_modules.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks, labels = dl_modules.tokenize_data(sentences, labels, tokenizer, MAX_LEN)\n",
    "\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original:', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = dl_modules.create_data_split(input_ids, attention_masks, labels, SEED_VAL)\n",
    "\n",
    "print('{:>5,} training samples'.format(len(train_dataset)))\n",
    "print('{:>6,} validation samples'.format(len(val_dataset)))\n",
    "print('{:>6,} test samples\\n'.format(len(test_dataset)))\n",
    "\n",
    "print('Whole dataset size:', len(train_dataset) + len(val_dataset) + len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dl_modules.create_data_loader(train_dataset, BATCH_SIZE)\n",
    "validation_dataloader = dl_modules.create_data_loader(val_dataset, BATCH_SIZE, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f'../datasets/02_LLM_datasets/{MODELTYPE}'):\n",
    "    os.makedirs(f'../datasets/02_LLM_datasets/{MODELTYPE}')\n",
    "\n",
    "if not os.path.exists(f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}'):\n",
    "    \n",
    "    os.makedirs(f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/training')\n",
    "    os.makedirs(f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/validation')\n",
    "    \n",
    "    for index, batch in enumerate(train_dataloader):\n",
    "        torch.save(batch, f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/training/batch_{index}.pt')\n",
    "        \n",
    "    for index, batch in enumerate(validation_dataloader):\n",
    "        torch.save(batch, f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/validation/validation_batch_{index}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete unused variables\n",
    "del sentences, labels, input_ids, attention_masks, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODELTYPE == \"transformer\":\n",
    "    \n",
    "    model = FunnelForSequenceClassification.from_pretrained(\n",
    "        model_mapping[MODELTYPE],\n",
    "        num_labels = 2, \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "    \n",
    "else:\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_mapping[MODELTYPE],\n",
    "        num_labels = 2, \n",
    "        output_attentions = False,\n",
    "        output_hidden_states = False\n",
    "    )\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  LEARNING_RATE, # default is 5e-5\n",
    "                  eps = 1e-8 # default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "length_train_dataloader = len(train_dataloader)\n",
    "length_val_dataloader = len(validation_dataloader)\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed_all(SEED_VAL)\n",
    "\n",
    "training_stats = []\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(f'======== Epoch {epoch_i + 1} / {EPOCHS} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step in range(length_train_dataloader):\n",
    "        \n",
    "        batch = torch.load(f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/training/batch_{step}.pt')\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 10 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = dl_modules.format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print(f'  Batch {step:>5,}  of  {length_train_dataloader:>5,}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        loss = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     labels=b_labels).loss\n",
    "        \n",
    "        logits = model(b_input_ids,\n",
    "                       token_type_ids=None,\n",
    "                       attention_mask=b_input_mask,\n",
    "                       labels=b_labels).logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_train_accuracy += dl_modules.flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average accuracy over all of the batches.\n",
    "    avg_train_accuracy = total_train_accuracy / length_train_dataloader\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / length_train_dataloader            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = dl_modules.format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"  Average training accuracy: {avg_train_accuracy:.2f}\")\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "    print(f\"  Training epoch took: {training_time:}\")\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    " \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for step in range(length_val_dataloader):\n",
    "        \n",
    "        batch = torch.load(f'../datasets/02_LLM_datasets/{MODELTYPE}/{DATASET}/{MAX_LEN}/validation/batch_{step}.pt')\n",
    "   \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            loss = model(b_input_ids, \n",
    "                         attention_mask=b_input_mask,\n",
    "                         labels=b_labels).loss\n",
    "            \n",
    "            logits = model(b_input_ids,\n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels).logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += dl_modules.flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / length_val_dataloader\n",
    "    print(\"  Average Validation Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / length_val_dataloader\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = dl_modules.format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'Epoch': epoch_i + 1,\n",
    "            'Training_Loss': avg_train_loss,\n",
    "            'Training_Accuracy': avg_train_accuracy,\n",
    "            'Validation_Loss': avg_val_loss,\n",
    "            'Validation_Accuracy': avg_val_accuracy,\n",
    "            'Training_Time': training_time,\n",
    "            'Validation_Time': validation_time\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "    \n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(\"../performance_statistics\"):\n",
    "        os.makedirs(\"../performance_statistics\")\n",
    "    \n",
    "    df_stats.to_csv(f\"../performance_statistics/{MODELTYPE}/training_stats_{MODELTYPE}_{LEARNING_RATE}_{MAX_LEN}_{DATASET}.csv\", index=False)\n",
    "    \n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(dl_modules.format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "output_dir = f'../models/'\n",
    "folder_name = f\"model_{MODELTYPE}_{LEARNING_RATE}_{MAX_LEN}_{DATASET}\"\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "output_dir = output_dir + folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to {output_dir}\")\n",
    "\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
