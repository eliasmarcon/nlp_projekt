{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f85ec92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bff6c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combine = pd.read_csv('../datasets/00_output_datasets/dataset_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4961f4b-5c98-4665-a04a-6283e35343e0",
   "metadata": {},
   "source": [
    "## Pre-processing standard Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faacc378-f338-41ea-b2b3-f2807ecdfb27",
   "metadata": {},
   "source": [
    "### Text Cleaning (Removing special characters & numbers and handling contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "457d2201-1c09-4d93-a57f-4ee94c931f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate trans people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate gay people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate black people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate disabled people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                    text\n",
       "0  hate            I hate women\n",
       "1  hate     I hate trans people\n",
       "2  hate       I hate gay people\n",
       "3  hate     I hate black people\n",
       "4  hate  I hate disabled people"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to clean text: remove special characters, numbers, and expand contractions\n",
    "def clean_text(text):\n",
    "    # Dictionary of English Contractions\n",
    "    contractions_dict = {\n",
    "        \"I'm\": \"I am\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"couldn't\": \"could not\"\n",
    "            # Add more contractions as needed\n",
    "    }\n",
    "    # Regular expression for finding contractions\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "    # Function for expanding contractions\n",
    "    def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "        def replace(match):\n",
    "            return contractions_dict[match.group(0)]\n",
    "        return contractions_re.sub(replace, s)\n",
    "\n",
    "    # Expand Contractions\n",
    "    text = expand_contractions(text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to both DataFrames\n",
    "df_combine['text'] = df_combine['text'].apply(clean_text)\n",
    "\n",
    "# Display the head of the combined DataFrame to verify the changes\n",
    "display(df_combine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6e7410-55c4-4a96-a1dc-e01047743a59",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90cd986a-8e7f-488d-84d0-5cec0b76003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # For tokenization\n",
    "nltk.download('wordnet')  # For lemmatization\n",
    "\n",
    "# Now, initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dd27b71-add0-4c42-bcbb-f3ce846466df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the 'omw-1.4' resource to fix an error that I encountered further down below\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2065896-84dd-45f1-a009-3c77cac375b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate trans people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate gay people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate black people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate disabled people</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                    text\n",
       "0  hate            i hate woman\n",
       "1  hate     i hate trans people\n",
       "2  hate       i hate gay people\n",
       "3  hate     i hate black people\n",
       "4  hate  i hate disabled people"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert text to lowercase\n",
    "df_combine['text'] = df_combine['text'].str.lower()\n",
    "\n",
    "# Initialize the NLTK WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize each word in the text\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Rejoin lemmatized tokens into a single string\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "# Apply the lemmatization function to the 'text' column\n",
    "df_combine['text'] = df_combine['text'].apply(lemmatize_text)\n",
    "\n",
    "# Display the head of the DataFrame to verify the changes\n",
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ce5bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_combine.iterrows():\n",
    "    if row['text'] == '':\n",
    "        df_combine.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "658d9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving for BERT Model which tokenizes the text on its own\n",
    "df_combine.to_csv('../datasets/01_preprocessed_datasets/dataset_preprocessed_no_transformation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4bc3d",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e4ce57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate woman</td>\n",
       "      <td>[i, hate, woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate trans people</td>\n",
       "      <td>[i, hate, trans, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate gay people</td>\n",
       "      <td>[i, hate, gay, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate black people</td>\n",
       "      <td>[i, hate, black, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>i hate disabled people</td>\n",
       "      <td>[i, hate, disabled, people]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                    text                       tokens\n",
       "0  hate            i hate woman             [i, hate, woman]\n",
       "1  hate     i hate trans people     [i, hate, trans, people]\n",
       "2  hate       i hate gay people       [i, hate, gay, people]\n",
       "3  hate     i hate black people     [i, hate, black, people]\n",
       "4  hate  i hate disabled people  [i, hate, disabled, people]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure you have the necessary NLTK resource downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Assuming df_combine is your DataFrame and it has a column named 'text' with normalized text\n",
    "# Define a function to tokenize text\n",
    "def tokenize_text(text):\n",
    "    # Use NLTK's word_tokenize function to split the text into tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenization function to each row in the 'text' column\n",
    "df_combine['tokens'] = df_combine['text'].apply(tokenize_text)\n",
    "\n",
    "# Display the first few rows to check the tokenized text\n",
    "display(df_combine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c6188",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e04b5ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3390c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    \"\"\"Remove stop words from a list of tokens\"\"\"\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "966b033e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate</td>\n",
       "      <td>[hate, woman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>[hate, trans, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>[hate, gay, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>[hate, black, people]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>[hate, disabled, people]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                      text\n",
       "0  hate             [hate, woman]\n",
       "1  hate     [hate, trans, people]\n",
       "2  hate       [hate, gay, people]\n",
       "3  hate     [hate, black, people]\n",
       "4  hate  [hate, disabled, people]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Applying it to tokens column\n",
    "df_combine['text'] = df_combine['tokens'].apply(remove_stop_words)\n",
    "\n",
    "df_combine.drop(columns=['tokens'], inplace=True)\n",
    "# Display the DataFrame to verify stop words are removed\n",
    "display(df_combine.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09aa63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving for BERT Model which tokenizes the text on its own\n",
    "df_combine.to_csv('../datasets/01_preprocessed_datasets/dataset_preprocessed_stopwords.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753b57f2-c334-43bc-a081-d7a14f5331af",
   "metadata": {},
   "source": [
    "### Vectorization (JAN PART BEGINS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5aeac-b9bf-41dc-8e22-dfb07a8135d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the tokens back into a string\n",
    "df_combine['processed_text'] = df_combine['tokens_filtered'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf7751-0a35-4373-8bc0-643d6d3a7d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44872, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for simplicity\n",
    "\n",
    "# Fit and transform the 'processed_text' column\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_combine['processed_text'])\n",
    "\n",
    "# tfidf_matrix is a sparse matrix of shape (n_samples, n_features)\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890aeaca-b8b6-4287-bb30-93d8c56bbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling imbalanced data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b2c549-0c84-4c7d-895c-102d69c1e0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
