{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate women.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate trans people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate gay people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate black people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                     text\n",
       "0  hate            I hate women.\n",
       "1  hate     I hate trans people.\n",
       "2  hate       I hate gay people.\n",
       "3  hate     I hate black people.\n",
       "4  hate  I hate disabled people."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "training_data_raw = pd.read_csv('../datasets/00_output_datasets/dataset_combined.csv')\n",
    "training_data_preprocessed = pd.read_csv('../datasets/01_preprocessed_datasets/dataset_preprocessed_stopwords.csv')\n",
    "\n",
    "training_data_raw.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_dataset = training_data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(chosen_dataset['text'], \n",
    "                                                    chosen_dataset['label'], \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Applying TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Applying TF-IDF Vectorization with bigrams\n",
    "tfidf_vectorizer_ngrams = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_train_tfidf_ngrams = tfidf_vectorizer_ngrams.fit_transform(X_train)\n",
    "X_test_tfidf_ngrams = tfidf_vectorizer_ngrams.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dimensions = 300\n",
    "glove_billion_tokens = 840\n",
    "\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rsplit(' ', glove_dimensions)\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "glove_path = f'../datasets/embeddings/glove/glove.{glove_billion_tokens}B.{glove_dimensions}d.txt'\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_path = '../datasets/embeddings/word2vec/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, embeddings, dimensions):\n",
    "    words = text.split()\n",
    "    vectors = [embeddings[word] for word in words if word in embeddings]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros((dimensions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighed_embeddings(text, model, vectorizer, dimensions):\n",
    "    words = text.split()\n",
    "    # Generate a dictionary of words and their TF-IDF scores\n",
    "    word2tfidf = {word: vectorizer.idf_[vectorizer.vocabulary_[word]] \n",
    "                  for word in words if word in vectorizer.vocabulary_}\n",
    "\n",
    "    # Compute the TF-IDF weighted average of word embeddings\n",
    "    weighted_vectors = np.zeros(dimensions)\n",
    "    total_weight = 0.0\n",
    "\n",
    "    for word, tfidf_value in word2tfidf.items():\n",
    "        if word in model:\n",
    "            weighted_vectors += model[word] * tfidf_value\n",
    "            total_weight += tfidf_value\n",
    "\n",
    "    # Handle out-of-vocabulary cases\n",
    "    if total_weight > 0:\n",
    "        weighted_vectors /= total_weight\n",
    "    else:\n",
    "        weighted_vectors = np.zeros(dimensions)\n",
    "\n",
    "    return weighted_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.array([vectorize_text(text, glove_embeddings, glove_dimensions) for text in X_train])\n",
    "X_test_glove = np.array([vectorize_text(text, glove_embeddings, glove_dimensions) for text in X_test])\n",
    "\n",
    "X_train_word2vec = np.array([vectorize_text(text, word2vec_model, word2vec_model.vector_size) for text in X_train])\n",
    "X_test_word2vec = np.array([vectorize_text(text, word2vec_model, word2vec_model.vector_size) for text in X_test])\n",
    "\n",
    "X_train_tfidf_glove = np.array([tfidf_weighed_embeddings(text, glove_embeddings, tfidf_vectorizer, glove_dimensions) for text in X_train])\n",
    "X_test_tfidf_glove = np.array([tfidf_weighed_embeddings(text, glove_embeddings, tfidf_vectorizer, glove_dimensions) for text in X_test])\n",
    "\n",
    "X_train_tfidf_word2vec = np.array([tfidf_weighed_embeddings(text, word2vec_model, tfidf_vectorizer, word2vec_model.vector_size) for text in X_train])\n",
    "X_test_tfidf_word2vec = np.array([tfidf_weighed_embeddings(text, word2vec_model, tfidf_vectorizer, word2vec_model.vector_size) for text in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def benchmark_models_vectorizations(vectorized_data_sets, y_train, y_test, use_grid_search=False, c_values=None):\n",
    "    \"\"\"\n",
    "    Train and evaluate Logistic Regression on multiple vectorized data sets.\n",
    "    Optionally performs GridSearch to find the best 'C' parameter.\n",
    "\n",
    "    Args:\n",
    "    - vectorized_data_sets (list of tuples): A list where each tuple contains vectorized training and testing data (X_train_vec, X_test_vec) along with a descriptor string.\n",
    "    - y_train (array-like): Training labels.\n",
    "    - y_test (array-like): Testing labels.\n",
    "    - use_grid_search (bool): Whether to use GridSearchCV to find the optimal 'C' parameter.\n",
    "    - c_values (list): List of 'C' values to try if GridSearch is enabled.\n",
    "\n",
    "    Returns:\n",
    "    - results (dict): A dictionary containing accuracy and classification report for each vectorization approach.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if use_grid_search and c_values is None:\n",
    "        c_values = [0.001, 0.01, 0.1, 1, 10, 100]  # Default C values if none provided\n",
    "\n",
    "    for vec_name, (X_train_vec, X_test_vec) in vectorized_data_sets:\n",
    "        if use_grid_search:\n",
    "            # Define the model and parameter grid\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            param_grid = {'C': c_values}\n",
    "            grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
    "            grid_search.fit(X_train_vec, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            y_pred = best_model.predict(X_test_vec)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            best_c = grid_search.best_params_['C']\n",
    "            results[vec_name] = {'Accuracy': accuracy, 'Classification_Report': report, 'Best_C': best_c}\n",
    "        else:\n",
    "            # Train the model without GridSearch\n",
    "            model = LogisticRegression(max_iter=1000)\n",
    "            model.fit(X_train_vec, y_train)\n",
    "            y_pred = model.predict(X_test_vec)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            results[vec_name] = {'Accuracy': accuracy, 'Classification_Report': report}\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      "Accuracy: 0.6861448780834577\n",
      "  hate - Precision: 0.70, Recall: 0.74, F1-Score: 0.72\n",
      "  not_hate - Precision: 0.66, Recall: 0.62, F1-Score: 0.64\n",
      "  macro avg - Precision: 0.68, Recall: 0.68, F1-Score: 0.68\n",
      "  weighted avg - Precision: 0.68, Recall: 0.69, F1-Score: 0.68\n",
      "\n",
      "TF-IDF with N-Grams:\n",
      "Accuracy: 0.617118077770669\n",
      "  hate - Precision: 0.64, Recall: 0.72, F1-Score: 0.67\n",
      "  not_hate - Precision: 0.59, Recall: 0.50, F1-Score: 0.54\n",
      "  macro avg - Precision: 0.61, Recall: 0.61, F1-Score: 0.61\n",
      "  weighted avg - Precision: 0.61, Recall: 0.62, F1-Score: 0.61\n",
      "\n",
      "GloVe:\n",
      "Accuracy: 0.6680173455605317\n",
      "  hate - Precision: 0.69, Recall: 0.73, F1-Score: 0.71\n",
      "  not_hate - Precision: 0.64, Recall: 0.59, F1-Score: 0.61\n",
      "  macro avg - Precision: 0.66, Recall: 0.66, F1-Score: 0.66\n",
      "  weighted avg - Precision: 0.67, Recall: 0.67, F1-Score: 0.67\n",
      "\n",
      "Word2Vec:\n",
      "Accuracy: 0.6615483045425464\n",
      "  hate - Precision: 0.67, Recall: 0.75, F1-Score: 0.71\n",
      "  not_hate - Precision: 0.64, Recall: 0.56, F1-Score: 0.60\n",
      "  macro avg - Precision: 0.66, Recall: 0.65, F1-Score: 0.65\n",
      "  weighted avg - Precision: 0.66, Recall: 0.66, F1-Score: 0.66\n",
      "\n",
      "TF-IDF weighted GloVe:\n",
      "Accuracy: 0.6597710954716713\n",
      "  hate - Precision: 0.68, Recall: 0.72, F1-Score: 0.70\n",
      "  not_hate - Precision: 0.63, Recall: 0.58, F1-Score: 0.61\n",
      "  macro avg - Precision: 0.66, Recall: 0.65, F1-Score: 0.65\n",
      "  weighted avg - Precision: 0.66, Recall: 0.66, F1-Score: 0.66\n",
      "\n",
      "TF-IDF weighted Word2Vec:\n",
      "Accuracy: 0.6500319897632757\n",
      "  hate - Precision: 0.66, Recall: 0.74, F1-Score: 0.70\n",
      "  not_hate - Precision: 0.63, Recall: 0.54, F1-Score: 0.58\n",
      "  macro avg - Precision: 0.65, Recall: 0.64, F1-Score: 0.64\n",
      "  weighted avg - Precision: 0.65, Recall: 0.65, F1-Score: 0.65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized_data_sets = [\n",
    "    ('TF-IDF', (X_train_tfidf, X_test_tfidf)),\n",
    "    ('TF-IDF with N-Grams', (X_train_tfidf_ngrams, X_test_tfidf_ngrams)),\n",
    "    ('GloVe', (X_train_glove, X_test_glove)),\n",
    "    ('Word2Vec', (X_train_word2vec, X_test_word2vec)),\n",
    "    ('TF-IDF weighted GloVe', (X_train_tfidf_glove, X_test_tfidf_glove)),\n",
    "    ('TF-IDF weighted Word2Vec', (X_train_tfidf_word2vec, X_test_tfidf_word2vec)),\n",
    "]\n",
    "\n",
    "results = benchmark_models_vectorizations(vectorized_data_sets, y_train, y_test, use_grid_search=True)\n",
    "\n",
    "for setup, result in results.items():\n",
    "    print(f\"{setup}:\")\n",
    "    print(f\"Accuracy: {result['Accuracy']}\")\n",
    "    for label, scores in result['Classification_Report'].items():\n",
    "        if isinstance(scores, dict):\n",
    "            print(f\"  {label} - Precision: {scores['precision']:.2f}, Recall: {scores['recall']:.2f}, F1-Score: {scores['f1-score']:.2f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
